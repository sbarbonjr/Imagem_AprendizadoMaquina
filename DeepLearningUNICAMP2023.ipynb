{"cells":[{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":411,"status":"ok","timestamp":1680608548997,"user":{"displayName":"Sylvio Barbon","userId":"17570083437711261042"},"user_tz":180},"id":"XJyFLmPo3RUm","outputId":"938c2742-71da-45d8-d663-83bc14200a58"},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'imagesDL' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/sbarbonjr/Imagem_AprendizadoMaquina/ imagesDL"]},{"cell_type":"code","execution_count":73,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680608549423,"user":{"displayName":"Sylvio Barbon","userId":"17570083437711261042"},"user_tz":180},"id":"LH2EaxEp37PU"},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","from matplotlib import image as mp_image\n","import seaborn as sns\n","\n","# Required magic to display matplotlib plots in notebooks\n","%matplotlib inline\n","\n","from sklearn import metrics\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","import shutil\n"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680608549423,"user":{"displayName":"Sylvio Barbon","userId":"17570083437711261042"},"user_tz":180},"id":"fLVVg4DO3-Sx","outputId":"f7c523b3-e552-4673-de15-741bf3ecd559"},"outputs":[{"name":"stdout","output_type":"stream","text":["['EM1', 'EM2', 'EM3']\n"]}],"source":["training_folder_name = 'imagesDL/DL'\n","img_size = (2592,1944)\n","\n","# The folder contains a subfolder for each class of shape\n","classes = sorted(os.listdir(training_folder_name))\n","print(classes)"]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1680608549423,"user":{"displayName":"Sylvio Barbon","userId":"17570083437711261042"},"user_tz":180},"id":"_dkVyLQn8YIV","outputId":"8e5834b0-e1ae-4861-a9fa-6fe9a32b6e2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Libraries imported - ready to use PyTorch 2.0.0+cu118\n"]}],"source":["# Import PyTorch libraries\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"]},{"cell_type":"code","execution_count":76,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1680608549423,"user":{"displayName":"Sylvio Barbon","userId":"17570083437711261042"},"user_tz":180},"id":"7ltqw5bf8ci8"},"outputs":[],"source":["from PIL import Image\n","\n","# function to resize image\n","def resize_image(src_image, size=(128,128), bg_color=\"white\"): \n","    from PIL import Image, ImageOps \n","    \n","    # resize the image so the longest dimension matches our target size\n","    src_image.thumbnail(size, Image.ANTIALIAS)\n","    \n","    # Create a new square background image\n","    new_image = Image.new(\"RGB\", size, bg_color)\n","    \n","    # Paste the resized image into the center of the square background\n","    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n","  \n","    # return the resized image\n","    return new_image"]},{"cell_type":"code","execution_count":77,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5232,"status":"ok","timestamp":1680608554653,"user":{"displayName":"Sylvio Barbon","userId":"17570083437711261042"},"user_tz":180},"id":"fOYvgCXf8i4S","outputId":"70fa99a5-d454-4760-8577-7ab8a6ae6ebc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Transforming images...\n","processing folder EM1\n","processing folder EM2\n","processing folder EM3\n","Done.\n"]}],"source":["# New location for the resized images\n","train_folder = 'new_size'\n","\n","\n","# Create resized copies of all of the source images\n","size = (128,128)\n","\n","# Create the output folder if it doesn't already exist\n","if os.path.exists(train_folder):\n","    shutil.rmtree(train_folder)\n","\n","# Loop through each subfolder in the input folder\n","print('Transforming images...')\n","for root, folders, files in os.walk(training_folder_name):\n","    for sub_folder in folders:\n","        print('processing folder ' + sub_folder)\n","        # Create a matching subfolder in the output dir\n","        saveFolder = os.path.join(train_folder,sub_folder)\n","        if not os.path.exists(saveFolder):\n","            os.makedirs(saveFolder)\n","        # Loop through the files in the subfolder\n","        file_names = os.listdir(os.path.join(root,sub_folder))\n","        for file_name in file_names:\n","            # Open the file\n","            file_path = os.path.join(root,sub_folder, file_name)\n","            #print(\"reading \" + file_path)\n","            image = Image.open(file_path)\n","            # Create a resized version and save it\n","            resized_image = resize_image(image, size)\n","            saveAs = os.path.join(saveFolder, file_name)\n","            #print(\"writing \" + saveAs)\n","            resized_image.save(saveAs)\n","\n","print('Done.')"]},{"cell_type":"code","execution_count":78,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1680608554653,"user":{"displayName":"Sylvio Barbon","userId":"17570083437711261042"},"user_tz":180},"id":"gOW1yaMl8_-9","outputId":"7e03fb8b-03c5-48ff-c542-0267dd7b12ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data loaders ready to read new_size\n"]}],"source":["def load_dataset(data_path):\n","    import torch\n","    import torchvision\n","    import torchvision.transforms as transforms\n","    # Load all the images\n","    transformation = transforms.Compose([\n","        # Randomly augment the image data\n","        # Random horizontal flip\n","        transforms.RandomHorizontalFlip(0.5),\n","        # Random vertical flip\n","        transforms.RandomVerticalFlip(0.3),\n","        # transform to tensors\n","        transforms.ToTensor(),\n","        # Normalize the pixel values (in R, G, and B channels)\n","        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","    ])\n","\n","    # Load all of the images, transforming them\n","    full_dataset = torchvision.datasets.ImageFolder(\n","        root=data_path,\n","        transform=transformation\n","    )\n","    \n","    \n","    # Split into training (70% and testing (30%) datasets)\n","    train_size = int(0.7 * len(full_dataset))\n","    test_size = len(full_dataset) - train_size\n","    \n","    # use torch.utils.data.random_split for training/test split\n","    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n","    \n","    # define a loader for the training data we can iterate through in 50-image batches\n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=20,\n","        num_workers=0,\n","        shuffle=False\n","    )\n","    \n","    # define a loader for the testing data we can iterate through in 50-image batches\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset,\n","        batch_size=20,\n","        num_workers=0,\n","        shuffle=False\n","    )\n","        \n","    return train_loader, test_loader\n","\n","\n","\n","\n","#####################################################################################################\n","\n","\n","\n","\n","# Recall that we have resized the images and saved them into\n","train_folder = 'new_size'\n","\n","# Get the iterative dataloaders for test and training data\n","train_loader, test_loader = load_dataset(train_folder)\n","batch_size = train_loader.batch_size\n","print(\"Data loaders ready to read\", train_folder)"]},{"cell_type":"code","execution_count":79,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680608554654,"user":{"displayName":"Sylvio Barbon","userId":"17570083437711261042"},"user_tz":180},"id":"usNYy_cq9K7u","outputId":"022e9bbf-7fd9-4a92-834a-d796916b1d92"},"outputs":[{"name":"stdout","output_type":"stream","text":["Net(\n","  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (drop): Dropout2d(p=0.3, inplace=False)\n","  (fc): Linear(in_features=24576, out_features=3, bias=True)\n",")\n"]}],"source":["# Create a neural net class\n","class Net(nn.Module):\n","    \n","    \n","    # Defining the Constructor\n","    def __init__(self, num_classes=3):\n","        super(Net, self).__init__()\n","        \n","        # Our images are RGB, so we have input channels = 3. \n","        # We will apply 12 filters in the first convolutional layer\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n","        \n","        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n","        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n","        \n","        # We in the end apply max pooling with a kernel size of 2\n","        self.pool = nn.MaxPool2d(kernel_size=2)\n","        \n","        # A drop layer deletes 20% of the features to help prevent overfitting\n","        self.drop = nn.Dropout2d(p=0.3)\n","        \n","        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n","        # This means that our feature tensors are now 32 x 32, and we've generated 24 of them\n","        \n","        # We need to flatten these in order to feed them to a fully-connected layer\n","        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n","\n","    def forward(self, x):\n","        # In the forward function, pass the data through the layers we defined in the init function\n","        \n","        #x =x.cuda()\n","        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n","        x = F.relu(self.pool(self.conv1(x))) \n","        \n","        # Use a ReLU activation function after layer 2\n","        x = F.relu(self.pool(self.conv2(x)))  \n","        \n","        # Select some features to drop to prevent overfitting (only drop during training)\n","        x = F.dropout(self.drop(x), training=self.training)\n","        \n","        # Flatten\n","        x = x.view(-1, 32 * 32 * 24)\n","        # Feed to fully-connected layer to predict class\n","        x = self.fc(x)\n","        # Return class probabilities via a log_softmax function \n","        return torch.log_softmax(x, dim=1)\n","    \n","device = \"cpu\"\n","if (torch.cuda.is_available()):\n","  #if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n","    device = \"cuda\"\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Create an instance of the model class and allocate it to the device\n","model = Net(num_classes=len(classes)).to(device)\n","\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"eyRykejCDze-"},"source":[]},{"cell_type":"code","execution_count":80,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1680608554654,"user":{"displayName":"Sylvio Barbon","userId":"17570083437711261042"},"user_tz":180},"id":"upn0c7iZ9PlN"},"outputs":[],"source":["def train(model, device, train_loader, optimizer, epoch):\n","    # Set the model to training mode\n","    model.train()\n","    train_loss = 0\n","    print(\"Epoch:\", epoch)\n","    # Process the images in batches\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        # Use the CPU or GPU as appropriate\n","        # Recall that GPU is optimized for the operations we are dealing with\n","        data, target = data.to(device), target.to(device)\n","        \n","        # Reset the optimizer\n","        optimizer.zero_grad()\n","        \n","        # Push the data forward through the model layers\n","        output = model(data)\n","        \n","        # Get the loss\n","        loss = loss_criteria(output, target)\n","\n","        # Keep a running total\n","        train_loss += loss.item()\n","        \n","        # Backpropagate\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Print metrics so we see some progress\n","        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n","            \n","    # return average loss for the epoch\n","    avg_loss = train_loss / (batch_idx+1)\n","    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n","    return avg_loss"]},{"cell_type":"code","execution_count":81,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1680608554654,"user":{"displayName":"Sylvio Barbon","userId":"17570083437711261042"},"user_tz":180},"id":"56d97wn09WIX"},"outputs":[],"source":["def test(model, device, test_loader):\n","    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        batch_count = 0\n","        for data, target in test_loader:\n","            batch_count += 1\n","            data, target = data.to(device), target.to(device)\n","            \n","            # Get the predicted classes for this batch\n","            output = model(data)\n","            \n","            # Calculate the loss for this batch\n","            test_loss += loss_criteria(output, target).item()\n","            \n","            # Calculate the accuracy for this batch\n","            _, predicted = torch.max(output.data, 1)\n","            correct += torch.sum(target==predicted).item()\n","\n","    # Calculate the average loss and total accuracy for this epoch\n","    avg_loss = test_loss / batch_count\n","    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        avg_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","    \n","    # return average loss for the epoch\n","    return avg_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"zG1tDWVm9Z2b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training on cpu\n","Epoch: 1\n","\tTraining batch 1 Loss: 1.098795\n","\tTraining batch 2 Loss: 29.382019\n","\tTraining batch 3 Loss: 9.777142\n","\tTraining batch 4 Loss: 1.279995\n","\tTraining batch 5 Loss: 1.115253\n","Training set: Average loss: 8.530641\n","Validation set: Average loss: 1.098472, Accuracy: 15/39 (38%)\n","\n","Epoch: 2\n","\tTraining batch 1 Loss: 1.102099\n","\tTraining batch 2 Loss: 1.099189\n","\tTraining batch 3 Loss: 1.097775\n","\tTraining batch 4 Loss: 1.097266\n","\tTraining batch 5 Loss: 1.098631\n","Training set: Average loss: 1.098992\n","Validation set: Average loss: 1.096334, Accuracy: 16/39 (41%)\n","\n","Epoch: 3\n","\tTraining batch 1 Loss: 1.092782\n","\tTraining batch 2 Loss: 1.092763\n","\tTraining batch 3 Loss: 1.094492\n","\tTraining batch 4 Loss: 1.093581\n","\tTraining batch 5 Loss: 1.097922\n","Training set: Average loss: 1.094308\n","Validation set: Average loss: 1.093527, Accuracy: 16/39 (41%)\n","\n","Epoch: 4\n","\tTraining batch 1 Loss: 1.080703\n","\tTraining batch 2 Loss: 1.084045\n","\tTraining batch 3 Loss: 1.089838\n","\tTraining batch 4 Loss: 1.088826\n","\tTraining batch 5 Loss: 1.097359\n","Training set: Average loss: 1.088154\n","Validation set: Average loss: 1.090638, Accuracy: 16/39 (41%)\n","\n","Epoch: 5\n","\tTraining batch 1 Loss: 1.066030\n","\tTraining batch 2 Loss: 1.074110\n","\tTraining batch 3 Loss: 1.084999\n","\tTraining batch 4 Loss: 1.084036\n","\tTraining batch 5 Loss: 1.097310\n","Training set: Average loss: 1.081297\n","Validation set: Average loss: 1.088196, Accuracy: 16/39 (41%)\n","\n","Epoch: 6\n","\tTraining batch 1 Loss: 1.050654\n","\tTraining batch 2 Loss: 1.064130\n","\tTraining batch 3 Loss: 1.080568\n","\tTraining batch 4 Loss: 1.079732\n","\tTraining batch 5 Loss: 1.097925\n","Training set: Average loss: 1.074602\n","Validation set: Average loss: 1.086445, Accuracy: 16/39 (41%)\n","\n","Epoch: 7\n","\tTraining batch 1 Loss: 1.035625\n","\tTraining batch 2 Loss: 1.054737\n","\tTraining batch 3 Loss: 1.076835\n","\tTraining batch 4 Loss: 1.076158\n","\tTraining batch 5 Loss: 1.099218\n","Training set: Average loss: 1.068515\n","Validation set: Average loss: 1.085453, Accuracy: 16/39 (41%)\n","\n","Epoch: 8\n","\tTraining batch 1 Loss: 1.021543\n","\tTraining batch 2 Loss: 1.046263\n","\tTraining batch 3 Loss: 1.073902\n","\tTraining batch 4 Loss: 1.073390\n","\tTraining batch 5 Loss: 1.101112\n","Training set: Average loss: 1.063242\n","Validation set: Average loss: 1.085183, Accuracy: 16/39 (41%)\n","\n","Epoch: 9\n","\tTraining batch 1 Loss: 1.008729\n","\tTraining batch 2 Loss: 1.038849\n","\tTraining batch 3 Loss: 1.071759\n","\tTraining batch 4 Loss: 1.071401\n","\tTraining batch 5 Loss: 1.103481\n","Training set: Average loss: 1.058844\n","Validation set: Average loss: 1.085534, Accuracy: 16/39 (41%)\n","\n","Epoch: 10\n","\tTraining batch 1 Loss: 0.997326\n","\tTraining batch 2 Loss: 1.032519\n","\tTraining batch 3 Loss: 1.070330\n","\tTraining batch 4 Loss: 1.070104\n","\tTraining batch 5 Loss: 1.106173\n","Training set: Average loss: 1.055290\n","Validation set: Average loss: 1.086371, Accuracy: 16/39 (41%)\n","\n","Epoch: 11\n","\tTraining batch 1 Loss: 0.987359\n","\tTraining batch 2 Loss: 1.027217\n","\tTraining batch 3 Loss: 1.069497\n","\tTraining batch 4 Loss: 1.069378\n","\tTraining batch 5 Loss: 1.109035\n","Training set: Average loss: 1.052497\n","Validation set: Average loss: 1.087552, Accuracy: 16/39 (41%)\n","\n","Epoch: 12\n","\tTraining batch 1 Loss: 0.978776\n","\tTraining batch 2 Loss: 1.022848\n","\tTraining batch 3 Loss: 1.069134\n","\tTraining batch 4 Loss: 1.069097\n","\tTraining batch 5 Loss: 1.111929\n","Training set: Average loss: 1.050357\n","Validation set: Average loss: 1.088941, Accuracy: 16/39 (41%)\n","\n","Epoch: 13\n","\tTraining batch 1 Loss: 0.971480\n","\tTraining batch 2 Loss: 1.019297\n","\tTraining batch 3 Loss: 1.069117\n","\tTraining batch 4 Loss: 1.069138\n","\tTraining batch 5 Loss: 1.114737\n","Training set: Average loss: 1.048754\n","Validation set: Average loss: 1.090421, Accuracy: 16/39 (41%)\n","\n","Epoch: 14\n","\tTraining batch 1 Loss: 0.965346\n","\tTraining batch 2 Loss: 1.016441\n","\tTraining batch 3 Loss: 1.069334\n","\tTraining batch 4 Loss: 1.069394\n","\tTraining batch 5 Loss: 1.117371\n","Training set: Average loss: 1.047577\n","Validation set: Average loss: 1.091898, Accuracy: 16/39 (41%)\n","\n","Epoch: 15\n","\tTraining batch 1 Loss: 0.960242\n","\tTraining batch 2 Loss: 1.014167\n","\tTraining batch 3 Loss: 1.069694\n","\tTraining batch 4 Loss: 1.069775\n","\tTraining batch 5 Loss: 1.119770\n","Training set: Average loss: 1.046730\n","Validation set: Average loss: 1.093302, Accuracy: 16/39 (41%)\n","\n","Epoch: 16\n","\tTraining batch 1 Loss: 0.956036\n","\tTraining batch 2 Loss: 1.012369\n","\tTraining batch 3 Loss: 1.070125\n","\tTraining batch 4 Loss: 1.070215\n","\tTraining batch 5 Loss: 1.121897\n","Training set: Average loss: 1.046128\n","Validation set: Average loss: 1.094585, Accuracy: 16/39 (41%)\n","\n","Epoch: 17\n","\tTraining batch 1 Loss: 0.952602\n","\tTraining batch 2 Loss: 1.010957\n","\tTraining batch 3 Loss: 1.070573\n","\tTraining batch 4 Loss: 1.070663\n","\tTraining batch 5 Loss: 1.123741\n","Training set: Average loss: 1.045707\n","Validation set: Average loss: 1.095721, Accuracy: 16/39 (41%)\n","\n","Epoch: 18\n","\tTraining batch 1 Loss: 0.949825\n","\tTraining batch 2 Loss: 1.009855\n","\tTraining batch 3 Loss: 1.071003\n","\tTraining batch 4 Loss: 1.071089\n","\tTraining batch 5 Loss: 1.125304\n","Training set: Average loss: 1.045415\n","Validation set: Average loss: 1.096699, Accuracy: 16/39 (41%)\n","\n","Epoch: 19\n","\tTraining batch 1 Loss: 0.947599\n","\tTraining batch 2 Loss: 1.009000\n","\tTraining batch 3 Loss: 1.071394\n","\tTraining batch 4 Loss: 1.071471\n","\tTraining batch 5 Loss: 1.126602\n","Training set: Average loss: 1.045213\n","Validation set: Average loss: 1.097520, Accuracy: 16/39 (41%)\n","\n","Epoch: 20\n","\tTraining batch 1 Loss: 0.945834\n","\tTraining batch 2 Loss: 1.008340\n","\tTraining batch 3 Loss: 1.071734\n","\tTraining batch 4 Loss: 1.071802\n","\tTraining batch 5 Loss: 1.127659\n","Training set: Average loss: 1.045074\n","Validation set: Average loss: 1.098194, Accuracy: 16/39 (41%)\n","\n","Epoch: 21\n","\tTraining batch 1 Loss: 0.944450\n","\tTraining batch 2 Loss: 1.007836\n","\tTraining batch 3 Loss: 1.072020\n","\tTraining batch 4 Loss: 1.072077\n","\tTraining batch 5 Loss: 1.128505\n","Training set: Average loss: 1.044978\n","Validation set: Average loss: 1.098734, Accuracy: 16/39 (41%)\n","\n","Epoch: 22\n","\tTraining batch 1 Loss: 0.943378\n","\tTraining batch 2 Loss: 1.007452\n","\tTraining batch 3 Loss: 1.072253\n","\tTraining batch 4 Loss: 1.072301\n","\tTraining batch 5 Loss: 1.129168\n","Training set: Average loss: 1.044910\n","Validation set: Average loss: 1.099158, Accuracy: 16/39 (41%)\n","\n","Epoch: 23\n","\tTraining batch 1 Loss: 0.942557\n","\tTraining batch 2 Loss: 1.007164\n","\tTraining batch 3 Loss: 1.072437\n","\tTraining batch 4 Loss: 1.072477\n","\tTraining batch 5 Loss: 1.129678\n","Training set: Average loss: 1.044863\n","Validation set: Average loss: 1.099484, Accuracy: 16/39 (41%)\n","\n","Epoch: 24\n","\tTraining batch 1 Loss: 0.941939\n","\tTraining batch 2 Loss: 1.006950\n","\tTraining batch 3 Loss: 1.072580\n","\tTraining batch 4 Loss: 1.072613\n","\tTraining batch 5 Loss: 1.130062\n","Training set: Average loss: 1.044829\n","Validation set: Average loss: 1.099729, Accuracy: 16/39 (41%)\n","\n","Epoch: 25\n","\tTraining batch 1 Loss: 0.941483\n","\tTraining batch 2 Loss: 1.006793\n","\tTraining batch 3 Loss: 1.072687\n","\tTraining batch 4 Loss: 1.072714\n","\tTraining batch 5 Loss: 1.130345\n","Training set: Average loss: 1.044804\n","Validation set: Average loss: 1.099907, Accuracy: 16/39 (41%)\n","\n","Epoch: 26\n","\tTraining batch 1 Loss: 0.941151\n","\tTraining batch 2 Loss: 1.006681\n","\tTraining batch 3 Loss: 1.072765\n","\tTraining batch 4 Loss: 1.072787\n","\tTraining batch 5 Loss: 1.130549\n","Training set: Average loss: 1.044787\n","Validation set: Average loss: 1.100034, Accuracy: 16/39 (41%)\n","\n","Epoch: 27\n","\tTraining batch 1 Loss: 0.940917\n","\tTraining batch 2 Loss: 1.006603\n","\tTraining batch 3 Loss: 1.072821\n","\tTraining batch 4 Loss: 1.072839\n","\tTraining batch 5 Loss: 1.130691\n","Training set: Average loss: 1.044774\n","Validation set: Average loss: 1.100121, Accuracy: 16/39 (41%)\n","\n","Epoch: 28\n","\tTraining batch 1 Loss: 0.940757\n","\tTraining batch 2 Loss: 1.006549\n","\tTraining batch 3 Loss: 1.072859\n","\tTraining batch 4 Loss: 1.072874\n","\tTraining batch 5 Loss: 1.130787\n","Training set: Average loss: 1.044765\n","Validation set: Average loss: 1.100178, Accuracy: 16/39 (41%)\n","\n","Epoch: 29\n","\tTraining batch 1 Loss: 0.940652\n","\tTraining batch 2 Loss: 1.006514\n","\tTraining batch 3 Loss: 1.072884\n","\tTraining batch 4 Loss: 1.072897\n","\tTraining batch 5 Loss: 1.130848\n","Training set: Average loss: 1.044759\n","Validation set: Average loss: 1.100213, Accuracy: 16/39 (41%)\n","\n","Epoch: 30\n","\tTraining batch 1 Loss: 0.940587\n","\tTraining batch 2 Loss: 1.006493\n","\tTraining batch 3 Loss: 1.072899\n","\tTraining batch 4 Loss: 1.072911\n","\tTraining batch 5 Loss: 1.130884\n","Training set: Average loss: 1.044755\n","Validation set: Average loss: 1.100232, Accuracy: 16/39 (41%)\n","\n","Epoch: 31\n","\tTraining batch 1 Loss: 0.940551\n","\tTraining batch 2 Loss: 1.006481\n","\tTraining batch 3 Loss: 1.072907\n","\tTraining batch 4 Loss: 1.072918\n","\tTraining batch 5 Loss: 1.130903\n","Training set: Average loss: 1.044752\n","Validation set: Average loss: 1.100240, Accuracy: 16/39 (41%)\n","\n","Epoch: 32\n","\tTraining batch 1 Loss: 0.940534\n","\tTraining batch 2 Loss: 1.006476\n","\tTraining batch 3 Loss: 1.072910\n","\tTraining batch 4 Loss: 1.072920\n","\tTraining batch 5 Loss: 1.130910\n","Training set: Average loss: 1.044750\n","Validation set: Average loss: 1.100241, Accuracy: 16/39 (41%)\n","\n","Epoch: 33\n","\tTraining batch 1 Loss: 0.940531\n","\tTraining batch 2 Loss: 1.006475\n","\tTraining batch 3 Loss: 1.072911\n","\tTraining batch 4 Loss: 1.072921\n","\tTraining batch 5 Loss: 1.130910\n","Training set: Average loss: 1.044749\n","Validation set: Average loss: 1.100237, Accuracy: 16/39 (41%)\n","\n","Epoch: 34\n","\tTraining batch 1 Loss: 0.940536\n","\tTraining batch 2 Loss: 1.006477\n","\tTraining batch 3 Loss: 1.072909\n","\tTraining batch 4 Loss: 1.072919\n","\tTraining batch 5 Loss: 1.130905\n","Training set: Average loss: 1.044749\n","Validation set: Average loss: 1.100231, Accuracy: 16/39 (41%)\n","\n","Epoch: 35\n","\tTraining batch 1 Loss: 0.940545\n","\tTraining batch 2 Loss: 1.006480\n","\tTraining batch 3 Loss: 1.072907\n","\tTraining batch 4 Loss: 1.072917\n","\tTraining batch 5 Loss: 1.130898\n","Training set: Average loss: 1.044749\n","Validation set: Average loss: 1.100224, Accuracy: 16/39 (41%)\n","\n","Epoch: 36\n","\tTraining batch 1 Loss: 0.940557\n","\tTraining batch 2 Loss: 1.006484\n","\tTraining batch 3 Loss: 1.072904\n","\tTraining batch 4 Loss: 1.072914\n","\tTraining batch 5 Loss: 1.130891\n","Training set: Average loss: 1.044750\n","Validation set: Average loss: 1.100217, Accuracy: 16/39 (41%)\n","\n","Epoch: 37\n","\tTraining batch 1 Loss: 0.940569\n","\tTraining batch 2 Loss: 1.006488\n","\tTraining batch 3 Loss: 1.072901\n","\tTraining batch 4 Loss: 1.072911\n","\tTraining batch 5 Loss: 1.130883\n","Training set: Average loss: 1.044750\n","Validation set: Average loss: 1.100210, Accuracy: 16/39 (41%)\n","\n","Epoch: 38\n","\tTraining batch 1 Loss: 0.940580\n","\tTraining batch 2 Loss: 1.006491\n","\tTraining batch 3 Loss: 1.072899\n","\tTraining batch 4 Loss: 1.072909\n","\tTraining batch 5 Loss: 1.130876\n","Training set: Average loss: 1.044751\n","Validation set: Average loss: 1.100204, Accuracy: 16/39 (41%)\n","\n","Epoch: 39\n","\tTraining batch 1 Loss: 0.940591\n","\tTraining batch 2 Loss: 1.006495\n","\tTraining batch 3 Loss: 1.072896\n","\tTraining batch 4 Loss: 1.072906\n","\tTraining batch 5 Loss: 1.130871\n","Training set: Average loss: 1.044752\n","Validation set: Average loss: 1.100198, Accuracy: 16/39 (41%)\n","\n","Epoch: 40\n","\tTraining batch 1 Loss: 0.940600\n","\tTraining batch 2 Loss: 1.006498\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072905\n","\tTraining batch 5 Loss: 1.130866\n","Training set: Average loss: 1.044753\n","Validation set: Average loss: 1.100194, Accuracy: 16/39 (41%)\n","\n","Epoch: 41\n","\tTraining batch 1 Loss: 0.940608\n","\tTraining batch 2 Loss: 1.006501\n","\tTraining batch 3 Loss: 1.072893\n","\tTraining batch 4 Loss: 1.072903\n","\tTraining batch 5 Loss: 1.130862\n","Training set: Average loss: 1.044753\n","Validation set: Average loss: 1.100190, Accuracy: 16/39 (41%)\n","\n","Epoch: 42\n","\tTraining batch 1 Loss: 0.940615\n","\tTraining batch 2 Loss: 1.006503\n","\tTraining batch 3 Loss: 1.072891\n","\tTraining batch 4 Loss: 1.072902\n","\tTraining batch 5 Loss: 1.130859\n","Training set: Average loss: 1.044754\n","Validation set: Average loss: 1.100187, Accuracy: 16/39 (41%)\n","\n","Epoch: 43\n","\tTraining batch 1 Loss: 0.940620\n","\tTraining batch 2 Loss: 1.006504\n","\tTraining batch 3 Loss: 1.072891\n","\tTraining batch 4 Loss: 1.072901\n","\tTraining batch 5 Loss: 1.130857\n","Training set: Average loss: 1.044755\n","Validation set: Average loss: 1.100185, Accuracy: 16/39 (41%)\n","\n","Epoch: 44\n","\tTraining batch 1 Loss: 0.940624\n","\tTraining batch 2 Loss: 1.006506\n","\tTraining batch 3 Loss: 1.072890\n","\tTraining batch 4 Loss: 1.072901\n","\tTraining batch 5 Loss: 1.130855\n","Training set: Average loss: 1.044755\n","Validation set: Average loss: 1.100183, Accuracy: 16/39 (41%)\n","\n","Epoch: 45\n","\tTraining batch 1 Loss: 0.940627\n","\tTraining batch 2 Loss: 1.006507\n","\tTraining batch 3 Loss: 1.072890\n","\tTraining batch 4 Loss: 1.072901\n","\tTraining batch 5 Loss: 1.130854\n","Training set: Average loss: 1.044756\n","Validation set: Average loss: 1.100181, Accuracy: 16/39 (41%)\n","\n","Epoch: 46\n","\tTraining batch 1 Loss: 0.940630\n","\tTraining batch 2 Loss: 1.006508\n","\tTraining batch 3 Loss: 1.072890\n","\tTraining batch 4 Loss: 1.072901\n","\tTraining batch 5 Loss: 1.130854\n","Training set: Average loss: 1.044756\n","Validation set: Average loss: 1.100180, Accuracy: 16/39 (41%)\n","\n","Epoch: 47\n","\tTraining batch 1 Loss: 0.940632\n","\tTraining batch 2 Loss: 1.006508\n","\tTraining batch 3 Loss: 1.072889\n","\tTraining batch 4 Loss: 1.072901\n","\tTraining batch 5 Loss: 1.130854\n","Training set: Average loss: 1.044757\n","Validation set: Average loss: 1.100179, Accuracy: 16/39 (41%)\n","\n","Epoch: 48\n","\tTraining batch 1 Loss: 0.940634\n","\tTraining batch 2 Loss: 1.006509\n","\tTraining batch 3 Loss: 1.072889\n","\tTraining batch 4 Loss: 1.072901\n","\tTraining batch 5 Loss: 1.130854\n","Training set: Average loss: 1.044757\n","Validation set: Average loss: 1.100178, Accuracy: 16/39 (41%)\n","\n","Epoch: 49\n","\tTraining batch 1 Loss: 0.940635\n","\tTraining batch 2 Loss: 1.006509\n","\tTraining batch 3 Loss: 1.072890\n","\tTraining batch 4 Loss: 1.072901\n","\tTraining batch 5 Loss: 1.130854\n","Training set: Average loss: 1.044758\n","Validation set: Average loss: 1.100177, Accuracy: 16/39 (41%)\n","\n","Epoch: 50\n","\tTraining batch 1 Loss: 0.940636\n","\tTraining batch 2 Loss: 1.006509\n","\tTraining batch 3 Loss: 1.072890\n","\tTraining batch 4 Loss: 1.072901\n","\tTraining batch 5 Loss: 1.130855\n","Training set: Average loss: 1.044758\n","Validation set: Average loss: 1.100177, Accuracy: 16/39 (41%)\n","\n","Epoch: 51\n","\tTraining batch 1 Loss: 0.940637\n","\tTraining batch 2 Loss: 1.006509\n","\tTraining batch 3 Loss: 1.072890\n","\tTraining batch 4 Loss: 1.072901\n","\tTraining batch 5 Loss: 1.130855\n","Training set: Average loss: 1.044759\n","Validation set: Average loss: 1.100176, Accuracy: 16/39 (41%)\n","\n","Epoch: 52\n","\tTraining batch 1 Loss: 0.940638\n","\tTraining batch 2 Loss: 1.006510\n","\tTraining batch 3 Loss: 1.072890\n","\tTraining batch 4 Loss: 1.072902\n","\tTraining batch 5 Loss: 1.130856\n","Training set: Average loss: 1.044759\n","Validation set: Average loss: 1.100176, Accuracy: 16/39 (41%)\n","\n","Epoch: 53\n","\tTraining batch 1 Loss: 0.940638\n","\tTraining batch 2 Loss: 1.006510\n","\tTraining batch 3 Loss: 1.072890\n","\tTraining batch 4 Loss: 1.072902\n","\tTraining batch 5 Loss: 1.130856\n","Training set: Average loss: 1.044759\n","Validation set: Average loss: 1.100175, Accuracy: 16/39 (41%)\n","\n","Epoch: 54\n","\tTraining batch 1 Loss: 0.940639\n","\tTraining batch 2 Loss: 1.006510\n","\tTraining batch 3 Loss: 1.072890\n","\tTraining batch 4 Loss: 1.072902\n","\tTraining batch 5 Loss: 1.130856\n","Training set: Average loss: 1.044759\n","Validation set: Average loss: 1.100175, Accuracy: 16/39 (41%)\n","\n","Epoch: 55\n","\tTraining batch 1 Loss: 0.940640\n","\tTraining batch 2 Loss: 1.006510\n","\tTraining batch 3 Loss: 1.072890\n","\tTraining batch 4 Loss: 1.072902\n","\tTraining batch 5 Loss: 1.130857\n","Training set: Average loss: 1.044760\n","Validation set: Average loss: 1.100174, Accuracy: 16/39 (41%)\n","\n","Epoch: 56\n","\tTraining batch 1 Loss: 0.940640\n","\tTraining batch 2 Loss: 1.006510\n","\tTraining batch 3 Loss: 1.072891\n","\tTraining batch 4 Loss: 1.072902\n","\tTraining batch 5 Loss: 1.130857\n","Training set: Average loss: 1.044760\n","Validation set: Average loss: 1.100174, Accuracy: 16/39 (41%)\n","\n","Epoch: 57\n","\tTraining batch 1 Loss: 0.940641\n","\tTraining batch 2 Loss: 1.006510\n","\tTraining batch 3 Loss: 1.072891\n","\tTraining batch 4 Loss: 1.072903\n","\tTraining batch 5 Loss: 1.130858\n","Training set: Average loss: 1.044761\n","Validation set: Average loss: 1.100174, Accuracy: 16/39 (41%)\n","\n","Epoch: 58\n","\tTraining batch 1 Loss: 0.940641\n","\tTraining batch 2 Loss: 1.006510\n","\tTraining batch 3 Loss: 1.072891\n","\tTraining batch 4 Loss: 1.072903\n","\tTraining batch 5 Loss: 1.130858\n","Training set: Average loss: 1.044761\n","Validation set: Average loss: 1.100173, Accuracy: 16/39 (41%)\n","\n","Epoch: 59\n","\tTraining batch 1 Loss: 0.940642\n","\tTraining batch 2 Loss: 1.006511\n","\tTraining batch 3 Loss: 1.072891\n","\tTraining batch 4 Loss: 1.072903\n","\tTraining batch 5 Loss: 1.130859\n","Training set: Average loss: 1.044761\n","Validation set: Average loss: 1.100173, Accuracy: 16/39 (41%)\n","\n","Epoch: 60\n","\tTraining batch 1 Loss: 0.940642\n","\tTraining batch 2 Loss: 1.006511\n","\tTraining batch 3 Loss: 1.072891\n","\tTraining batch 4 Loss: 1.072903\n","\tTraining batch 5 Loss: 1.130859\n","Training set: Average loss: 1.044761\n","Validation set: Average loss: 1.100172, Accuracy: 16/39 (41%)\n","\n","Epoch: 61\n","\tTraining batch 1 Loss: 0.940643\n","\tTraining batch 2 Loss: 1.006511\n","\tTraining batch 3 Loss: 1.072891\n","\tTraining batch 4 Loss: 1.072903\n","\tTraining batch 5 Loss: 1.130859\n","Training set: Average loss: 1.044762\n","Validation set: Average loss: 1.100172, Accuracy: 16/39 (41%)\n","\n","Epoch: 62\n","\tTraining batch 1 Loss: 0.940644\n","\tTraining batch 2 Loss: 1.006511\n","\tTraining batch 3 Loss: 1.072891\n","\tTraining batch 4 Loss: 1.072903\n","\tTraining batch 5 Loss: 1.130859\n","Training set: Average loss: 1.044762\n","Validation set: Average loss: 1.100171, Accuracy: 16/39 (41%)\n","\n","Epoch: 63\n","\tTraining batch 1 Loss: 0.940644\n","\tTraining batch 2 Loss: 1.006511\n","\tTraining batch 3 Loss: 1.072892\n","\tTraining batch 4 Loss: 1.072903\n","\tTraining batch 5 Loss: 1.130860\n","Training set: Average loss: 1.044762\n","Validation set: Average loss: 1.100171, Accuracy: 16/39 (41%)\n","\n","Epoch: 64\n","\tTraining batch 1 Loss: 0.940645\n","\tTraining batch 2 Loss: 1.006511\n","\tTraining batch 3 Loss: 1.072892\n","\tTraining batch 4 Loss: 1.072903\n","\tTraining batch 5 Loss: 1.130860\n","Training set: Average loss: 1.044762\n","Validation set: Average loss: 1.100171, Accuracy: 16/39 (41%)\n","\n","Epoch: 65\n","\tTraining batch 1 Loss: 0.940645\n","\tTraining batch 2 Loss: 1.006512\n","\tTraining batch 3 Loss: 1.072892\n","\tTraining batch 4 Loss: 1.072904\n","\tTraining batch 5 Loss: 1.130860\n","Training set: Average loss: 1.044763\n","Validation set: Average loss: 1.100170, Accuracy: 16/39 (41%)\n","\n","Epoch: 66\n","\tTraining batch 1 Loss: 0.940646\n","\tTraining batch 2 Loss: 1.006512\n","\tTraining batch 3 Loss: 1.072892\n","\tTraining batch 4 Loss: 1.072904\n","\tTraining batch 5 Loss: 1.130861\n","Training set: Average loss: 1.044763\n","Validation set: Average loss: 1.100170, Accuracy: 16/39 (41%)\n","\n","Epoch: 67\n","\tTraining batch 1 Loss: 0.940646\n","\tTraining batch 2 Loss: 1.006512\n","\tTraining batch 3 Loss: 1.072892\n","\tTraining batch 4 Loss: 1.072904\n","\tTraining batch 5 Loss: 1.130861\n","Training set: Average loss: 1.044763\n","Validation set: Average loss: 1.100170, Accuracy: 16/39 (41%)\n","\n","Epoch: 68\n","\tTraining batch 1 Loss: 0.940647\n","\tTraining batch 2 Loss: 1.006512\n","\tTraining batch 3 Loss: 1.072892\n","\tTraining batch 4 Loss: 1.072904\n","\tTraining batch 5 Loss: 1.130861\n","Training set: Average loss: 1.044763\n","Validation set: Average loss: 1.100169, Accuracy: 16/39 (41%)\n","\n","Epoch: 69\n","\tTraining batch 1 Loss: 0.940647\n","\tTraining batch 2 Loss: 1.006512\n","\tTraining batch 3 Loss: 1.072892\n","\tTraining batch 4 Loss: 1.072904\n","\tTraining batch 5 Loss: 1.130862\n","Training set: Average loss: 1.044764\n","Validation set: Average loss: 1.100169, Accuracy: 16/39 (41%)\n","\n","Epoch: 70\n","\tTraining batch 1 Loss: 0.940648\n","\tTraining batch 2 Loss: 1.006512\n","\tTraining batch 3 Loss: 1.072892\n","\tTraining batch 4 Loss: 1.072904\n","\tTraining batch 5 Loss: 1.130862\n","Training set: Average loss: 1.044764\n","Validation set: Average loss: 1.100169, Accuracy: 16/39 (41%)\n","\n","Epoch: 71\n","\tTraining batch 1 Loss: 0.940648\n","\tTraining batch 2 Loss: 1.006512\n","\tTraining batch 3 Loss: 1.072892\n","\tTraining batch 4 Loss: 1.072904\n","\tTraining batch 5 Loss: 1.130862\n","Training set: Average loss: 1.044764\n","Validation set: Average loss: 1.100168, Accuracy: 16/39 (41%)\n","\n","Epoch: 72\n","\tTraining batch 1 Loss: 0.940649\n","\tTraining batch 2 Loss: 1.006512\n","\tTraining batch 3 Loss: 1.072893\n","\tTraining batch 4 Loss: 1.072905\n","\tTraining batch 5 Loss: 1.130862\n","Training set: Average loss: 1.044764\n","Validation set: Average loss: 1.100168, Accuracy: 16/39 (41%)\n","\n","Epoch: 73\n","\tTraining batch 1 Loss: 0.940649\n","\tTraining batch 2 Loss: 1.006513\n","\tTraining batch 3 Loss: 1.072893\n","\tTraining batch 4 Loss: 1.072905\n","\tTraining batch 5 Loss: 1.130863\n","Training set: Average loss: 1.044764\n","Validation set: Average loss: 1.100168, Accuracy: 16/39 (41%)\n","\n","Epoch: 74\n","\tTraining batch 1 Loss: 0.940650\n","\tTraining batch 2 Loss: 1.006513\n","\tTraining batch 3 Loss: 1.072892\n","\tTraining batch 4 Loss: 1.072905\n","\tTraining batch 5 Loss: 1.130863\n","Training set: Average loss: 1.044765\n","Validation set: Average loss: 1.100167, Accuracy: 16/39 (41%)\n","\n","Epoch: 75\n","\tTraining batch 1 Loss: 0.940650\n","\tTraining batch 2 Loss: 1.006513\n","\tTraining batch 3 Loss: 1.072893\n","\tTraining batch 4 Loss: 1.072905\n","\tTraining batch 5 Loss: 1.130863\n","Training set: Average loss: 1.044765\n","Validation set: Average loss: 1.100167, Accuracy: 16/39 (41%)\n","\n","Epoch: 76\n","\tTraining batch 1 Loss: 0.940651\n","\tTraining batch 2 Loss: 1.006513\n","\tTraining batch 3 Loss: 1.072893\n","\tTraining batch 4 Loss: 1.072905\n","\tTraining batch 5 Loss: 1.130863\n","Training set: Average loss: 1.044765\n","Validation set: Average loss: 1.100167, Accuracy: 16/39 (41%)\n","\n","Epoch: 77\n","\tTraining batch 1 Loss: 0.940651\n","\tTraining batch 2 Loss: 1.006513\n","\tTraining batch 3 Loss: 1.072893\n","\tTraining batch 4 Loss: 1.072905\n","\tTraining batch 5 Loss: 1.130864\n","Training set: Average loss: 1.044765\n","Validation set: Average loss: 1.100166, Accuracy: 16/39 (41%)\n","\n","Epoch: 78\n","\tTraining batch 1 Loss: 0.940652\n","\tTraining batch 2 Loss: 1.006513\n","\tTraining batch 3 Loss: 1.072893\n","\tTraining batch 4 Loss: 1.072905\n","\tTraining batch 5 Loss: 1.130864\n","Training set: Average loss: 1.044765\n","Validation set: Average loss: 1.100166, Accuracy: 16/39 (41%)\n","\n","Epoch: 79\n","\tTraining batch 1 Loss: 0.940652\n","\tTraining batch 2 Loss: 1.006513\n","\tTraining batch 3 Loss: 1.072893\n","\tTraining batch 4 Loss: 1.072905\n","\tTraining batch 5 Loss: 1.130864\n","Training set: Average loss: 1.044765\n","Validation set: Average loss: 1.100166, Accuracy: 16/39 (41%)\n","\n","Epoch: 80\n","\tTraining batch 1 Loss: 0.940652\n","\tTraining batch 2 Loss: 1.006513\n","\tTraining batch 3 Loss: 1.072893\n","\tTraining batch 4 Loss: 1.072906\n","\tTraining batch 5 Loss: 1.130864\n","Training set: Average loss: 1.044766\n","Validation set: Average loss: 1.100166, Accuracy: 16/39 (41%)\n","\n","Epoch: 81\n","\tTraining batch 1 Loss: 0.940653\n","\tTraining batch 2 Loss: 1.006514\n","\tTraining batch 3 Loss: 1.072893\n","\tTraining batch 4 Loss: 1.072906\n","\tTraining batch 5 Loss: 1.130865\n","Training set: Average loss: 1.044766\n","Validation set: Average loss: 1.100165, Accuracy: 16/39 (41%)\n","\n","Epoch: 82\n","\tTraining batch 1 Loss: 0.940653\n","\tTraining batch 2 Loss: 1.006514\n","\tTraining batch 3 Loss: 1.072893\n","\tTraining batch 4 Loss: 1.072906\n","\tTraining batch 5 Loss: 1.130865\n","Training set: Average loss: 1.044766\n","Validation set: Average loss: 1.100165, Accuracy: 16/39 (41%)\n","\n","Epoch: 83\n","\tTraining batch 1 Loss: 0.940654\n","\tTraining batch 2 Loss: 1.006514\n","\tTraining batch 3 Loss: 1.072893\n","\tTraining batch 4 Loss: 1.072906\n","\tTraining batch 5 Loss: 1.130865\n","Training set: Average loss: 1.044766\n","Validation set: Average loss: 1.100165, Accuracy: 16/39 (41%)\n","\n","Epoch: 84\n","\tTraining batch 1 Loss: 0.940654\n","\tTraining batch 2 Loss: 1.006514\n","\tTraining batch 3 Loss: 1.072893\n","\tTraining batch 4 Loss: 1.072906\n","\tTraining batch 5 Loss: 1.130865\n","Training set: Average loss: 1.044766\n","Validation set: Average loss: 1.100165, Accuracy: 16/39 (41%)\n","\n","Epoch: 85\n","\tTraining batch 1 Loss: 0.940654\n","\tTraining batch 2 Loss: 1.006514\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072906\n","\tTraining batch 5 Loss: 1.130865\n","Training set: Average loss: 1.044766\n","Validation set: Average loss: 1.100164, Accuracy: 16/39 (41%)\n","\n","Epoch: 86\n","\tTraining batch 1 Loss: 0.940655\n","\tTraining batch 2 Loss: 1.006514\n","\tTraining batch 3 Loss: 1.072893\n","\tTraining batch 4 Loss: 1.072906\n","\tTraining batch 5 Loss: 1.130866\n","Training set: Average loss: 1.044767\n","Validation set: Average loss: 1.100164, Accuracy: 16/39 (41%)\n","\n","Epoch: 87\n","\tTraining batch 1 Loss: 0.940655\n","\tTraining batch 2 Loss: 1.006514\n","\tTraining batch 3 Loss: 1.072893\n","\tTraining batch 4 Loss: 1.072906\n","\tTraining batch 5 Loss: 1.130866\n","Training set: Average loss: 1.044767\n","Validation set: Average loss: 1.100164, Accuracy: 16/39 (41%)\n","\n","Epoch: 88\n","\tTraining batch 1 Loss: 0.940655\n","\tTraining batch 2 Loss: 1.006514\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072906\n","\tTraining batch 5 Loss: 1.130866\n","Training set: Average loss: 1.044767\n","Validation set: Average loss: 1.100164, Accuracy: 16/39 (41%)\n","\n","Epoch: 89\n","\tTraining batch 1 Loss: 0.940656\n","\tTraining batch 2 Loss: 1.006514\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072906\n","\tTraining batch 5 Loss: 1.130866\n","Training set: Average loss: 1.044767\n","Validation set: Average loss: 1.100163, Accuracy: 16/39 (41%)\n","\n","Epoch: 90\n","\tTraining batch 1 Loss: 0.940656\n","\tTraining batch 2 Loss: 1.006514\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072906\n","\tTraining batch 5 Loss: 1.130867\n","Training set: Average loss: 1.044767\n","Validation set: Average loss: 1.100163, Accuracy: 16/39 (41%)\n","\n","Epoch: 91\n","\tTraining batch 1 Loss: 0.940656\n","\tTraining batch 2 Loss: 1.006514\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072906\n","\tTraining batch 5 Loss: 1.130866\n","Training set: Average loss: 1.044767\n","Validation set: Average loss: 1.100163, Accuracy: 16/39 (41%)\n","\n","Epoch: 92\n","\tTraining batch 1 Loss: 0.940657\n","\tTraining batch 2 Loss: 1.006514\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072907\n","\tTraining batch 5 Loss: 1.130866\n","Training set: Average loss: 1.044768\n","Validation set: Average loss: 1.100163, Accuracy: 16/39 (41%)\n","\n","Epoch: 93\n","\tTraining batch 1 Loss: 0.940657\n","\tTraining batch 2 Loss: 1.006515\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072906\n","\tTraining batch 5 Loss: 1.130867\n","Training set: Average loss: 1.044768\n","Validation set: Average loss: 1.100163, Accuracy: 16/39 (41%)\n","\n","Epoch: 94\n","\tTraining batch 1 Loss: 0.940657\n","\tTraining batch 2 Loss: 1.006515\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072906\n","\tTraining batch 5 Loss: 1.130867\n","Training set: Average loss: 1.044768\n","Validation set: Average loss: 1.100162, Accuracy: 16/39 (41%)\n","\n","Epoch: 95\n","\tTraining batch 1 Loss: 0.940657\n","\tTraining batch 2 Loss: 1.006515\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072907\n","\tTraining batch 5 Loss: 1.130867\n","Training set: Average loss: 1.044768\n","Validation set: Average loss: 1.100162, Accuracy: 16/39 (41%)\n","\n","Epoch: 96\n","\tTraining batch 1 Loss: 0.940658\n","\tTraining batch 2 Loss: 1.006515\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072907\n","\tTraining batch 5 Loss: 1.130867\n","Training set: Average loss: 1.044768\n","Validation set: Average loss: 1.100162, Accuracy: 16/39 (41%)\n","\n","Epoch: 97\n","\tTraining batch 1 Loss: 0.940658\n","\tTraining batch 2 Loss: 1.006515\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072907\n","\tTraining batch 5 Loss: 1.130867\n","Training set: Average loss: 1.044768\n","Validation set: Average loss: 1.100162, Accuracy: 16/39 (41%)\n","\n","Epoch: 98\n","\tTraining batch 1 Loss: 0.940658\n","\tTraining batch 2 Loss: 1.006515\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072907\n","\tTraining batch 5 Loss: 1.130867\n","Training set: Average loss: 1.044768\n","Validation set: Average loss: 1.100161, Accuracy: 16/39 (41%)\n","\n","Epoch: 99\n","\tTraining batch 1 Loss: 0.940659\n","\tTraining batch 2 Loss: 1.006515\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072907\n","\tTraining batch 5 Loss: 1.130868\n","Training set: Average loss: 1.044769\n","Validation set: Average loss: 1.100161, Accuracy: 16/39 (41%)\n","\n","Epoch: 100\n","\tTraining batch 1 Loss: 0.940659\n","\tTraining batch 2 Loss: 1.006515\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072907\n","\tTraining batch 5 Loss: 1.130868\n","Training set: Average loss: 1.044769\n","Validation set: Average loss: 1.100161, Accuracy: 16/39 (41%)\n","\n","Epoch: 101\n","\tTraining batch 1 Loss: 0.940659\n","\tTraining batch 2 Loss: 1.006515\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072907\n","\tTraining batch 5 Loss: 1.130868\n","Training set: Average loss: 1.044769\n","Validation set: Average loss: 1.100161, Accuracy: 16/39 (41%)\n","\n","Epoch: 102\n","\tTraining batch 1 Loss: 0.940659\n","\tTraining batch 2 Loss: 1.006515\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072907\n","\tTraining batch 5 Loss: 1.130868\n","Training set: Average loss: 1.044769\n","Validation set: Average loss: 1.100161, Accuracy: 16/39 (41%)\n","\n","Epoch: 103\n","\tTraining batch 1 Loss: 0.940660\n","\tTraining batch 2 Loss: 1.006515\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072907\n","\tTraining batch 5 Loss: 1.130868\n","Training set: Average loss: 1.044769\n","Validation set: Average loss: 1.100161, Accuracy: 16/39 (41%)\n","\n","Epoch: 104\n","\tTraining batch 1 Loss: 0.940660\n","\tTraining batch 2 Loss: 1.006515\n","\tTraining batch 3 Loss: 1.072894\n","\tTraining batch 4 Loss: 1.072907\n","\tTraining batch 5 Loss: 1.130868\n","Training set: Average loss: 1.044769\n","Validation set: Average loss: 1.100160, Accuracy: 16/39 (41%)\n","\n","Epoch: 105\n","\tTraining batch 1 Loss: 0.940660\n","\tTraining batch 2 Loss: 1.006515\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072907\n","\tTraining batch 5 Loss: 1.130869\n","Training set: Average loss: 1.044769\n","Validation set: Average loss: 1.100160, Accuracy: 16/39 (41%)\n","\n","Epoch: 106\n","\tTraining batch 1 Loss: 0.940661\n","\tTraining batch 2 Loss: 1.006516\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072907\n","\tTraining batch 5 Loss: 1.130869\n","Training set: Average loss: 1.044769\n","Validation set: Average loss: 1.100160, Accuracy: 16/39 (41%)\n","\n","Epoch: 107\n","\tTraining batch 1 Loss: 0.940661\n","\tTraining batch 2 Loss: 1.006516\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072908\n","\tTraining batch 5 Loss: 1.130869\n","Training set: Average loss: 1.044769\n","Validation set: Average loss: 1.100160, Accuracy: 16/39 (41%)\n","\n","Epoch: 108\n","\tTraining batch 1 Loss: 0.940661\n","\tTraining batch 2 Loss: 1.006516\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072907\n","\tTraining batch 5 Loss: 1.130869\n","Training set: Average loss: 1.044770\n","Validation set: Average loss: 1.100160, Accuracy: 16/39 (41%)\n","\n","Epoch: 109\n","\tTraining batch 1 Loss: 0.940661\n","\tTraining batch 2 Loss: 1.006516\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072908\n","\tTraining batch 5 Loss: 1.130869\n","Training set: Average loss: 1.044770\n","Validation set: Average loss: 1.100159, Accuracy: 16/39 (41%)\n","\n","Epoch: 110\n","\tTraining batch 1 Loss: 0.940661\n","\tTraining batch 2 Loss: 1.006516\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072908\n","\tTraining batch 5 Loss: 1.130869\n","Training set: Average loss: 1.044770\n","Validation set: Average loss: 1.100160, Accuracy: 16/39 (41%)\n","\n","Epoch: 111\n","\tTraining batch 1 Loss: 0.940662\n","\tTraining batch 2 Loss: 1.006516\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072908\n","\tTraining batch 5 Loss: 1.130869\n","Training set: Average loss: 1.044770\n","Validation set: Average loss: 1.100159, Accuracy: 16/39 (41%)\n","\n","Epoch: 112\n","\tTraining batch 1 Loss: 0.940662\n","\tTraining batch 2 Loss: 1.006516\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072908\n","\tTraining batch 5 Loss: 1.130869\n","Training set: Average loss: 1.044770\n","Validation set: Average loss: 1.100159, Accuracy: 16/39 (41%)\n","\n","Epoch: 113\n","\tTraining batch 1 Loss: 0.940662\n","\tTraining batch 2 Loss: 1.006516\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072908\n","\tTraining batch 5 Loss: 1.130870\n","Training set: Average loss: 1.044770\n","Validation set: Average loss: 1.100159, Accuracy: 16/39 (41%)\n","\n","Epoch: 114\n","\tTraining batch 1 Loss: 0.940662\n","\tTraining batch 2 Loss: 1.006516\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072908\n","\tTraining batch 5 Loss: 1.130870\n","Training set: Average loss: 1.044770\n","Validation set: Average loss: 1.100159, Accuracy: 16/39 (41%)\n","\n","Epoch: 115\n","\tTraining batch 1 Loss: 0.940663\n","\tTraining batch 2 Loss: 1.006516\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072908\n","\tTraining batch 5 Loss: 1.130870\n","Training set: Average loss: 1.044770\n","Validation set: Average loss: 1.100159, Accuracy: 16/39 (41%)\n","\n","Epoch: 116\n","\tTraining batch 1 Loss: 0.940663\n","\tTraining batch 2 Loss: 1.006516\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072908\n","\tTraining batch 5 Loss: 1.130870\n","Training set: Average loss: 1.044770\n","Validation set: Average loss: 1.100159, Accuracy: 16/39 (41%)\n","\n","Epoch: 117\n","\tTraining batch 1 Loss: 0.940663\n","\tTraining batch 2 Loss: 1.006516\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072908\n","\tTraining batch 5 Loss: 1.130870\n","Training set: Average loss: 1.044770\n","Validation set: Average loss: 1.100158, Accuracy: 16/39 (41%)\n","\n","Epoch: 118\n","\tTraining batch 1 Loss: 0.940663\n","\tTraining batch 2 Loss: 1.006516\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072908\n","\tTraining batch 5 Loss: 1.130870\n","Training set: Average loss: 1.044770\n","Validation set: Average loss: 1.100158, Accuracy: 16/39 (41%)\n","\n","Epoch: 119\n","\tTraining batch 1 Loss: 0.940663\n","\tTraining batch 2 Loss: 1.006516\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072908\n","\tTraining batch 5 Loss: 1.130870\n","Training set: Average loss: 1.044771\n","Validation set: Average loss: 1.100158, Accuracy: 16/39 (41%)\n","\n","Epoch: 120\n","\tTraining batch 1 Loss: 0.940663\n","\tTraining batch 2 Loss: 1.006516\n","\tTraining batch 3 Loss: 1.072895\n","\tTraining batch 4 Loss: 1.072908\n","\tTraining batch 5 Loss: 1.130870\n","Training set: Average loss: 1.044771\n","Validation set: Average loss: 1.100158, Accuracy: 16/39 (41%)\n","\n"]}],"source":["# Use an \"Adam\" optimizer to adjust weights\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","# Specify the loss criteria\n","loss_criteria = nn.CrossEntropyLoss()\n","\n","# Track metrics in these arrays\n","epoch_nums = []\n","training_loss = []\n","validation_loss = []\n","\n","# Train over 10 epochs (We restrict to 10 for time issues)\n","epochs = 120\n","print('Training on', device)\n","for epoch in range(1, epochs + 1):\n","        train_loss = train(model, device, train_loader, optimizer, epoch)\n","        test_loss = test(model, device, test_loader)\n","        epoch_nums.append(epoch)\n","        training_loss.append(train_loss)\n","        validation_loss.append(test_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVDlcwtu9gcx"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.plot(epoch_nums, training_loss)\n","plt.plot(epoch_nums, validation_loss)\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.legend(['training', 'validation'], loc='upper right')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YIkvFOI9kbR"},"outputs":[],"source":["# Defining Labels and Predictions\n","truelabels = []\n","predictions = []\n","model.eval()\n","print(\"Getting predictions from test set...\")\n","for data, target in test_loader:\n","    for label in target.data.numpy():\n","        truelabels.append(label)\n","    for prediction in model(data).data.numpy().argmax(1):\n","        predictions.append(prediction) \n","\n","# Plot the confusion matrix\n","cm = confusion_matrix(truelabels, predictions)\n","tick_marks = np.arange(len(classes))\n","\n","df_cm = pd.DataFrame(cm, index = classes, columns = classes)\n","plt.figure(figsize = (7,7))\n","sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n","plt.xlabel(\"Predicted Shape\", fontsize = 20)\n","plt.ylabel(\"True Shape\", fontsize = 20)\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOVm51GT1asp8xVetEO8CkI","name":"","toc_visible":true,"version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}